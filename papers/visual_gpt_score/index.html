<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />

<html>
  <head>
    <title>Revisiting the Role of Language Priors in Vision-Language Models</title>
    <meta property="og:title" content="VisualGPTScore" />
    <style>
      .softgray {
          background-color: rgb(230, 230, 230);
      }
      
      .softblue {
          background-color: rgb(224, 235, 255);
      }
      
      .softgreen {
          background-color: rgb(224, 255, 224);
      }
      
      .softyellow {
          background-color: rgb(255, 255, 224);
      }
      
      .softred {
          background-color: rgb(255, 224, 224);
      }
      
      .softpink {
          background-color: rgb(255, 224, 239);
      }
      
      .highlight {
          padding: 0.1em;
          line-height: 1.5;
      }
  </style>
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">Revisiting the Role of Language Priors in Vision-Language Models
</span>
    </center>

    <br><br>
      <table align=center width=800px>
      <tr>
        <td align=center width=100px>
           <span style="font-size:20px"><a href="http://linzhiqiu.github.io">Zhiqiu Lin*<sup>1</sup></a></span>
         </td>
        <td align=center width=100px>
          <span style="font-size:20px"><a href="https://www.linkedin.com/in/xinyue-chen-073a4114b/">Xinyue Chen*<sup>1</sup></a></span>
        </td>
        <td align=center width=100px>
          <span style="font-size:20px"><a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak<sup>1</sup></a></span>
        </td>
        <td align=center width=110px>
          <span style="font-size:20px"><a href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang<sup>2</sup></a></span>
        </td>
        <td align=center width=100px>
          <center>
          <span style="font-size:20px"><a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan<sup>1</sup></a></span>
          </center>
        </td>
     </tr>
     </table>

    <table align=center width=700px>
      <tr>
        <td align=center width=100px>
           <span style="font-size:20px">Carnegie Mellon University<sup>1</sup></span>
         </td>
        <td align=center width=100px>
          <span style="font-size:20px">Meta<sup>2</sup></span>
        </td>
      </tr>
    </table>

    <hr>
    

    <!-- <center>
      <h1>CVPR 2022 Presentation</h1>
    </center>
    <table align=center width=900px>
      <tr>
        <td width=600px>
          <center>
            <div class="video">
              <iframe width="720" height="405" src="https://www.youtube.com/embed/_Rc72L2njcs" frameborder="0"
                allowfullscreen></iframe>
            </div>
          </center>
        </td>
      </tr>
    </table>
      <br><hr> -->
      <table align=center width=1000px>
        <tr>
          <center>
            <h1>Abstract</h1>
          </center>
            <td><img style="width:500px" src="./images/teaser_small_new.png" /></td>
        </tr>
        
      </table>
        <!-- <br> -->
    <p align=left style="width:1000px; padding: 0px 0px 0px 60px">
      Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, 
      without any fine-tuning. We study <i>generative VLMs</i> that are trained for next-word generation given an image. We explore their zero-shot performance on the 
      illustrative task of image-text retrieval across 8 popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks 
      (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the <i>Visual Generative Pre-Training Score</i> (VisualGPTScore). 
      While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. 
      We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions 
      by creating adversarial but unlikely text captions. In fact, we demonstrate that even a "blind" language model that ignores any image evidence 
      can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many 
      years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at 
      test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a 
      strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.
      </p>
      <table align=center width=400px>
        <tr>
          <td align=center width=100px>
            <span style="font-size:20px; text-align:center;"><a href="http://arxiv.org/abs/2306.01879">[Arxiv]</a></span>
          </td>
          <td align=center width=100px>
            <center>
              <span style="font-size:20px"><a href="https://github.com/linzhiqiu/visual_gpt_score">[Code]</a></span>
            </center>
          </td>
        </tr>
      </table>

      <br>

      <hr>
      <table align=center width=800>
       <center><h1>Generative Models for Discriminative Tasks</h1></center>
       <p align=left style="width:1000px; padding: 0px 0px 0px 60px">
        While the performance of vision-language models (VLMs) is impressive, many open challenges remain. Recent analysis points out that VLMs may often degrade to "bag-of-words" that confuse captions such as <i>"the horse is eating the grass"</i> and <i>"the grass is eating the horse"</i>. 
        This makes it difficult to use VLMs to capture <b>compositions</b> of objects, attributes, and their relations. But somewhat interestingly, large-scale language models (LLMs) trained for autoregressive next-token prediction like GPT are able to discern such distinctions. 
        A related but under-appreciated difficulty is that of <i>benchmarking</i> the performance of visio-linguistic reasoning. Perhaps the most well-known example in the community is that of the influential VQA benchmarks, which could be largely solved by exploiting linguistic biases in the dataset -- 
        concretely, questions about images could often be answered by "blind" language-only models that did not look at the image. In particular, we examine this by revisiting the role of <b>language priors</b> (P(text)) in VLMs. 
        Ironically, we find that blind language-only models still excel on many image-text retrieval tasks that assess compositional reasoning. To address these challenges, we propose a probablistic treatment by repurposing generative VLMs for discriminative tasks (like retrieval).
        In particular, we set the match score for an image-text pair to be the probability that the VLM would generate that text from the given image, or P(text|image). 
        We call this probability score the Visual Generative Pre-Training Score, or VisualGPTScore. We observe that the VisualGPTScore performs surprisingly well on many benchmarks, e.g., producing near-perfect accuracy on ARO. However, it still struggles on other benchmarks such as Winoground. 
        We analyze this performance discrepancy through a probabilistic lens by deriving the language prior P(text) from VLMs via Monte-Carlo sampling.</p>
         <!-- <center><img style="width:500px" src="./images/tab1.png" /></center> -->
         <center><img style="width:800px" src="./images/method_new.png" /></center>
         <br>
      </table>

      <hr>
      <table align=center width=800>
       <center><h1>The Role of Language Priors in VLMs</h1></center>
       <p align=left style="width:1000px; padding: 0px 0px 0px 60px">
        Our key insight is that many benchmark biases can be formalized as <b>mismatching distributions over text</b> between train and test data - P<sub>train</sub>(text) versus P<sub>test</sub>(text). 
        We use a first-principles analysis to account for distribution shift by simply reweighting the VisualGPTScore with the Bayes factor P<sub>test</sub>(text)/P<sub>train</sub>(text), a process we call <span class="italic">debiasing.</span> 
        To compute the Bayes reweighting factor, we need access to both the train and test language prior. We compute P<sub>train</sub>(text) from an OTS VLM by drawing Monte-Carlo samples of P<sub>train</sub>(text|image) from trainset or Guassian noise images. 
        Because P<sub>test</sub>(text) may require access to the test set, we explore simplifying assumptions that it is (a) identical to P<sub>train</sub>(text) (<b>Scenario 1</b>), (b) uninformative/uniform (<b>Scenario 2</b>), or (c) tunable from a held-out val set. Our analysis helps explain the strong performance of the VisualGPTScore on certain benchmarks and its poor performance on others. 
        Moreover, our analysis offers simple strategies to improve performance through debiasing. 
         <!-- <center><img style="width:500px" src="./images/tab1.png" /></center> -->
         <center><img style="width:800px" src="./images/teaser_new.png" /></center>
         <p align=left style="width:1000px; padding: 0px 0px 0px 60px"><strong>Scenario 1 (left)</strong> constructs negative captions by shuffling words in the true caption (as in ARO-Flickr), but this produces implausible text such as <code>white a duck spreads its wings in while the water</code>. Here, exploiting the language bias of the training set will help since it will downweight the match score for such implausible negative captions. In fact, we show that a blind language-only model can easily identify the correct caption. <strong>Scenario 2 (right)</strong> constructs negative captions that are curated to be plausible (as in SugarCrepe). Here, the language bias of the training set may hurt, since it will prefer to match common captions that score well under the language prior; i.e., the incorrect caption of <code>people are cooking in a kitchen</code> is more likely than the true caption of <code>people are posing in a kitchen</code> under the language prior, and so removing the language bias improves performance.</p>
         <br>
      </table>

      <hr>
      <table align=center width=800>
       <center><h1>SOTA performance across ARO/Crepe/SugarCrepe/VL-CheckList</h1></center>
       <p align=left style="width:1000px; padding: 0px 0px 0px 60px">We implement OTS VisualGPTScore using the open-source image-conditioned language model BLIP, and achieve SOTA performance on all recent image-to-text (I-to-T) retrieval tasks, oftentimes surpassing prior art by a great margin. Notably, these approaches typically requires costly fine-tuning of CLIP with much more curated data, e.g., DAC use multiple foundation models including ChatGPT and SAM to perform data augmentation.</p> 
         <center><img style="width:800px" src="./images/perf_1.png" /></center>
         <center><img style="width:800px" src="./images/perf_2.png" /></center>
         <p>We begin by evaluating <span class="highlight softred">blind language models (in red)</span>. Surprisingly, this already produces SOTA accuracy on certain benchmarks such as ARO-Flickr, compared to the <span class="highlight softgray">best discriminative approaches (in gray)</span>. We also find that blind inference of generative VLMs, <span class="highlight softblue">P<sub>train</sub>(<b>t</b>) via sampling Gaussian noise images (in blue)</span>, often performs better and achieve above-chance performance even on the most recent SugarCrepe. Next, we show that simply repurposing a generative VLM's language generation head for computing image-text scores <span class="highlight softyellow">(VisualGPTScore in yellow)</span>, which corresponds to α = 0, consistently produces SOTA accuracy across all benchmarks. Finally, debiasing this score by <span class="highlight softgreen">tuning α on val set (in green)</span> further improves performance, establishing the new SOTA.</p>
         <!-- <p align=left style="width:1000px; padding: 0px 0px 0px 60px">By approximating P(text) with Monte Carlo sampling, we show that some benchmarks such as ARO and Crepe can be partially addressed via P(text). Importantly, solutions that ignore images can still outperform SOTA algorithms trained on carefully-tuned negative samples such as <a href="https://arxiv.org/pdf/2210.01936.pdf">NegCLIP</a>. This makes it hard to interpret the progress these methods have made in bridging the visio-linguistic compositionality gap. 
         </p> 
         <center><img style="width:800px" src="./images/tab2.png" /></center>
         <p align=left style="width:1000px; padding: 0px 0px 0px 60px">Meanwhile, PMI, the "debiased" version of VisualGPTScore can significantly boost its performance on balanced benchmarks such as Winoground and EqBen. Intuitively, this debiasing procedure mitigates the tendency of VisualGPTScore to always assign
          higher scores (regardless of the image) to more "common" texts (like "the person on top of the world")
          compared to less "common" texts (like "the world on top of the person"), since both texts have the
          same chance of being positive in Winoground testset. -->
        </p> 
        <br>
      </table>

      <hr>
      <table align=center width=800>
        <center>
          <h1>Additional Experimental Results</h1>
        </center>
        <!-- <tr> -->
        <p align=left style="width:1000px; padding: 0px 0px 0px 60px">We show that our novel debiasing solution based on Gaussian Noise images can consistently improve VisualGPTScore on both balanced compositionality (Winoground/EqBen) and large-scale retrieval benchmarks (COCO/Flickr30K). </p>
        <center><img style="width:800px" src="./images/winoground_coco.png" /></center>
        <p align=left style="width:1000px; padding: 0px 0px 0px 60px">While OTS generative scores do not work well, debiasing with a larger α close to 1 can consistently and often significantly improve I-to-T performance. To highlight the improvement, we mark <span class="highlight softyellow">results without debiasing (α=0) (in yellow)</span>, <span class="highlight softpink">debiasing with a fixed α=1 (in pink)</span>, and <span class="highlight softgreen">cross-validation using held-out val sets (α=α<sub>val</sub>*) (in green)</span>. </p>
        <p align=left style="width:1000px; padding: 0px 0px 0px 60px">Furthermore, OTS VisualGPTScore achieves robust text-to-image (T-to-I) retrieval performance across the board, competitive with the ITMScore. </p>
        <center><img style="width:400px" src="./images/t_to_i.png" /></center>
        <!-- <p align=left style="width:1000px; padding: 0px 0px 0px 60px">More ablation studies can be found in our paper. </p> -->
        <!-- <p align=left style="width:1000px; padding: 0px 0px 0px 60px">When increasing alpha from 0 to 1, we observe that performance decreases the most for datasets like COCO-Order and
          Flickr-Order, which are constructed with adversarial negative captions whose P<sub>train</sub>(text) are close to 0 and
          can satisfy the first scenario in a trivial fashion. </p>
        <center><img style="width:800px" src="./images/alpha_tune_chart.png" /></center> -->
      <!-- </tr> -->
        <!-- <center>
        <tr>
          <td><img style="width:565px" src="./images/sota.png" /></td>
          <td><img style="width:400px" src="./images/augment_sota.png" /></td>
        </tr>
        </center> -->

        <!-- <tr><p align=left style="width:1000px; padding: 0px 0px 0px 60px">In addition, we show in paper our approach also achieves the best training efficiency and test-time robustness to distribution shifts (e.g., from ImageNet to ImageNetV2).</p></tr> -->
      
      </table>
  

      <!-- <hr>
      <table align=center width=800>
       <center><h1>Debiasing significantly enhances Image-to-Text Retrieval Performance</h1></center>
       <p align=left style="width:1000px; padding: 0px 0px 0px 60px">Interestingly, the above alpha-tuning equation can be rewritten using the language of PMI<sup>k</sup>, a well-known variant of PMI that controls the amount of debiasing.       </p> 
       <center><img style="width:700px" src="./images/alpha_tune_2.png" /></center>
       <p align=left style="width:1000px; padding: 0px 0px 0px 60px">By tuning alpha on held-out validation sets, we can significantly improve the image-to-text retrieval results of VisualGPTScore on both classic and compositionality benchmarks.</p> 
       <center><img style="width:700px" src="./images/alpha_tune_result_2.png" /></center>
      <br> -->

      

          
      </table>

      <!-- <br> -->
      
      <hr>
      <table align=center width=800>
        <center>
          <h1>Limitations and Future Work</h1>
        </center>
        <p align=left style="width:1000px; padding: 0px 0px 0px 60px">
          Our analysis is based on simplified assumptions. For instance, the model might not accurately represent P<sub>train</sub>(text|image), a phenomenon we examine in our paper. Estimating P<sub>train</sub>(text) by sampling gaussian noise images is potentially imprecise, and we encourage future VLMs to directly model P<sub>train</sub>(text) or explore better techniques for debiasing.</p>
        <!-- <center><img style="width:400px" src="./images/code.png" /></center> -->
        <br> 
      
      
      
      
      </table>

      <br>
      <hr>
      <table align=center width=900>
       <center><h1>Paper</h1></center>
          <tr>
            <td><a href="./images/teaser_small_new.png"><img style="width:200px" src="./images/teaser_small_new.png"/></a></td>
            <td><span style="font-size:14pt">Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, Deva Ramanan.<br>
                <i>Revisiting the Role of Language Priors in Vision-Language Models</i><br>
              In submission.<br>
                <a href="http://arxiv.org/abs/2306.01879">[Arxiv]</a> &nbsp; &nbsp;
                <a href="https://github.com/linzhiqiu/visual_gpt_score">[Code]</a> &nbsp; &nbsp;
            </td>
          
          </tr>
            
      </table> <br>
      <!-- <center>Bibtex:</center> <br>
      <div align=center style="width:800">
      <pre><code>@article{linmultimodality,
      title={Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models},
      author={Lin, Zhiqiu and Yu, Samuel and Kuang, Zhiyi and Pathak, Deepak and Ramanan, Deva}
      }</code></pre></div>


      <br> -->
    <hr>


      <table align=center width=1100px>
        <tr>
          <td>
            <left>
              <center>
                <h1>Acknowledgements</h1>
              </center>
              This research was supported by CMU Argo AI Center for Autonomous Vehicle Research. We thank Yu Tong Tiffany for graphical design.
            </left>
          </td>
        </tr>
      </table>

      <br><br>
</body>
</html>
