<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Revisiting the Role of Language Priors in Vision-Language Models</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Revisiting the Role of Language Priors in Vision-Language Models
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="http://linzhiqiu.github.io" target="_blank">Zhiqiu Lin</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/xinyue-chen-073a4114b" target="_blank">Xinyue Chen</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">Deepak Pathak</a><sup>1</sup>,</span>
              <span class="author-block">
                <a target="_blank" href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">CMU<sup>1</sup>&nbsp;&nbsp&nbsp;&nbsp Meta<sup>2</sup><br></span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2306.01879" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/linzhiqiu/visual_gpt_score" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2306.01879" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, 
                without any fine-tuning. We study <i>generative VLMs</i> that are trained for next-word generation given an image. We explore their zero-shot performance on the 
                illustrative task of image-text retrieval across 8 popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks 
                (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the <i>Visual Generative Pre-Training Score</i> (VisualGPTScore). 
                While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. 
                We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions 
                by creating adversarial but unlikely text captions. In fact, we demonstrate that even a "blind" language model that ignores any image evidence 
                can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many 
                years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at 
                test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a 
                strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.
            </p>
          </div>
        </div>
      </div>
      <img src="./images/teaser_small_new.png" alt="VisualGPTScore"
                style="width: 750px; height: auto; display: block; margin: 0 auto;">
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Method Overview -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Generative Models for Discriminative Tasks</h2>
          <div class="content has-text-justified">

            <div class="item">
            

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                While the performance of vision-language models (VLMs) is impressive, many open challenges remain. Recent analysis points out that VLMs may often degrade to "bag-of-words" that confuse captions such as <i>"the horse is eating the grass"</i> and <i>"the grass is eating the horse"</i>. 
        This makes it difficult to use VLMs to capture <b>compositions</b> of objects, attributes, and their relations. But somewhat interestingly, large-scale language models (LLMs) trained for autoregressive next-token prediction like GPT are able to discern such distinctions. 
        A related but under-appreciated difficulty is that of <i>benchmarking</i> the performance of visio-linguistic reasoning. Perhaps the most well-known example in the community is that of the influential VQA benchmarks, which could be largely solved by exploiting linguistic biases in the dataset -- 
        concretely, questions about images could often be answered by "blind" language-only models that did not look at the image. In particular, we examine this by revisiting the role of <b>language priors</b> (P(text)) in VLMs. 
        Ironically, we find that blind language-only models still excel on many image-text retrieval tasks that assess compositional reasoning. To address these challenges, we propose a probablistic treatment by repurposing generative VLMs for discriminative tasks (like retrieval).
        In particular, we set the match score for an image-text pair to be the probability that the VLM would generate that text from the given image, or P(text|image). 
        We call this probability score the Visual Generative Pre-Training Score, or VisualGPTScore. We observe that the VisualGPTScore performs surprisingly well on many benchmarks, e.g., producing near-perfect accuracy on ARO. However, it still struggles on other benchmarks such as Winoground. 
        We analyze this performance discrepancy through a probabilistic lens by deriving the language prior P(text) from VLMs via Monte-Carlo sampling.</p>
              </p>
            </div>
          </div>
        </div>
      </div>
      <!-- Your image here -->
      <img src="./images/method_new.png" alt="VisualGPTScore Method"
      style="width: 850px; height: auto; display: block; margin: 0 auto;">
    </div>
  </section>
  <!-- End Method Overview -->


  <!-- Text-Image generation -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">The Role of Language Priors in VLMs</h2>
          <div class="content has-text-justified">

            <div class="item">
                
                <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                    Our key insight is that many benchmark biases can be formalized as <b>mismatching distributions over text</b> between train and test data - P<sub>train</sub>(text) versus P<sub>test</sub>(text). 
                    We use a first-principles analysis to account for distribution shift by simply reweighting the VisualGPTScore with the Bayes factor P<sub>test</sub>(text)/P<sub>train</sub>(text), a process we call <span class="italic">debiasing.</span> 
                    To compute the Bayes reweighting factor, we need access to both the train and test language prior. We compute P<sub>train</sub>(text) from an OTS VLM by drawing Monte-Carlo samples of P<sub>train</sub>(text|image) from trainset or Guassian noise images. 
                    Because P<sub>test</sub>(text) may require access to the test set, we explore simplifying assumptions that it is (a) identical to P<sub>train</sub>(text) (<b>Scenario 1</b>), (b) uninformative/uniform (<b>Scenario 2</b>), or (c) tunable from a held-out val set. Our analysis helps explain the strong performance of the VisualGPTScore on certain benchmarks and its poor performance on others. 
                    Moreover, our analysis offers simple strategies to improve performance through debiasing. 
                </p>
                <!-- Your image here -->
                <img src="./images/teaser_new.png" alt="VisualGPTScore Method"
                style="width: 850px; height: auto; display: block; margin: 0 auto;">

                <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;"><strong>Scenario 1 (left)</strong> constructs negative captions by shuffling words in the true caption (as in ARO-Flickr), but this produces implausible text such as <code>white a duck spreads its wings in while the water</code>. Here, exploiting the language bias of the training set will help since it will downweight the match score for such implausible negative captions. In fact, we show that a blind language-only model can easily identify the correct caption. <strong>Scenario 2 (right)</strong> constructs negative captions that are curated to be plausible (as in SugarCrepe). Here, the language bias of the training set may hurt, since it will prefer to match common captions that score well under the language prior; i.e., the incorrect caption of <code>people are cooking in a kitchen</code> is more likely than the true caption of <code>people are posing in a kitchen</code> under the language prior, and so removing the language bias improves performance.</p>
            </div>


          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->


  <!-- Prompt Inversion -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">SOTA performance across ARO/Crepe/SugarCrepe/VL-CheckList</h2>
          <div class="content has-text-justified">
            <p>
                We implement OTS VisualGPTScore using the open-source image-conditioned language model BLIP, and achieve SOTA performance on all recent image-to-text (I-to-T) retrieval tasks, oftentimes surpassing prior art by a great margin. Notably, these approaches typically requires costly fine-tuning of CLIP with much more curated data, e.g., DAC use multiple foundation models including ChatGPT and SAM to perform data augmentation.
            </p>
            <div class="item">
              <!-- Your image here -->
              <img src="./images/perf_1.png" alt=""
                style="max-width: 95%; height: auto; display: block; margin: 0 auto;">
              <img src="./images/perf_2.png" alt=""
                style="max-width: 95%; height: auto; display: block; margin: 0 auto;">

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                  We begin by evaluating <span style="color: red;">blind language models (in red)</span>. Surprisingly, this already produces SOTA accuracy on certain benchmarks such as ARO-Flickr, compared to the <span style="color: gray;">best discriminative approaches (in gray)</span>. We also find that blind inference of generative VLMs, <span style="color: blue;">P<sub>train</sub>(<b>t</b>) via sampling Gaussian noise images (in blue)</span>, often performs better and achieve above-chance performance even on the most recent SugarCrepe. Next, we show that simply repurposing a generative VLM's language generation head for computing image-text scores <span style="color: rgb(191, 191, 5);">(VisualGPTScore in yellow)</span>, which corresponds to α = 0, consistently produces SOTA accuracy across all benchmarks. Finally, debiasing this score by <span class="highlight softgreen">tuning α on val set (in green)</span> further improves performance, establishing the new SOTA.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Prompt Inversion -->

  <!-- Quantitative Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Additional Experimental Results</h2>
          <div class="content has-text-justified">

            <div class="item">
              <!-- Your image here -->
              
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
                  We show that our novel debiasing solution based on Gaussian Noise images can consistently improve VisualGPTScore on both balanced compositionality (Winoground/EqBen) and large-scale retrieval benchmarks (COCO/Flickr30K/ImageNet). 
                </p>
              <img src="./images/winoground_coco.png" alt="Image showing Winoground/COCO/ImageNet results" />
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">While OTS generative scores do not work well, debiasing with a larger α close to 1 can consistently and often significantly improve I-to-T performance. To highlight the improvement, we mark <span style="color: rgb(177, 177, 4);">results without debiasing (α=0) (in yellow)</span>, <span style="color: pink;">debiasing with a fixed α=1 (in pink)</span>, and <span style="color: green;">cross-validation using held-out val sets (α=α<sub>val</sub>*) (in green)
              <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">Furthermore, OTS VisualGPTScore achieves robust text-to-image (T-to-I) retrieval performance across the board, competitive with the ITMScore. </p>
              <img src="./images/t_to_i.png" alt="Image showing T2I results" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Quantitative Results -->

  <!-- Qualitative Results -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Limitations and Future Work</h2>
          <div class="content has-text-justified">
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">Our analysis is based on simplified assumptions. For instance, the model might not accurately represent P<sub>train</sub>(text|image), a phenomenon we examine in our paper. Estimating P<sub>train</sub>(text) by sampling gaussian noise images is potentially imprecise, and we encourage future VLMs to directly model P<sub>train</sub>(text) or explore better techniques for debiasing.</p>

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Qualitative Results -->

  <!--BibTex citation -->
  <section class="section hero is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{lin2023revisiting,
        title={Revisiting the Role of Language Priors in Vision-Language Models},
        author={Lin, Zhiqiu and Chen, Xinyue and Pathak, Deepak and Zhang, Pengchuan and Ramanan, Deva},
        journal={arXiv preprint arXiv:2306.01879},
        year={2023}
    }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->



  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
